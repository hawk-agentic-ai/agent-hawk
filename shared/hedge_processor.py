"""
Unified Hedge Fund Processor - Core business logic for both FastAPI and MCP server
Combines PromptIntelligenceEngine and SmartDataExtractor with universal processing logic
Extracted from unified_smart_backend.py for shared use
"""

import asyncio
import json
import os
import logging
import time
import hashlib
from typing import Dict, List, Optional, Any
from datetime import datetime

from .business_logic import PromptIntelligenceEngine, PromptAnalysisResult, PromptIntent, analyze_prompt
from .data_extractor import SmartDataExtractor  
from .supabase_client import DatabaseManager
from .cache_manager import get_cache_duration, OPTIMIZATION_STATS

logger = logging.getLogger(__name__)


class HedgeFundProcessor:
    """Unified processor for hedge fund operations - shared between FastAPI and MCP"""
    
    def __init__(self):
        self.db_manager = None
        self.supabase_client = None
        self.redis_client = None
        self.prompt_engine = None
        self.data_extractor = None
        self.table_hints = None
        
        # Performance tracking
        self.total_requests = 0
        self.total_processing_time = 0
    
    async def initialize(self):
        """Initialize all components for shared use"""
        try:
            # Initialize database connections
            self.db_manager = DatabaseManager()
            self.supabase_client, self.redis_client = await self.db_manager.initialize_connections()
            
            # Initialize intelligent modules
            self.prompt_engine = PromptIntelligenceEngine()
            self.data_extractor = SmartDataExtractor(self.supabase_client, self.redis_client)

            # Load optional table hints (non-blocking guardrails)
            try:
                hints_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config', 'table_hints.json')
                if os.path.exists(hints_path):
                    with open(hints_path, 'r', encoding='utf-8') as f:
                        self.table_hints = json.load(f)
                    logger.info("Table hints loaded for guardrails")
                else:
                    self.table_hints = None
            except Exception as e:
                logger.warning(f"Failed to load table hints: {e}")
                self.table_hints = None
            
            logger.info("HedgeFundProcessor initialized successfully")
            logger.info(f"Components: Supabase={'connected' if self.supabase_client else 'disconnected'}, Redis={'connected' if self.redis_client else 'disconnected'}")
            
        except Exception as e:
            logger.error(f"HedgeFundProcessor initialization error: {e}")
            raise
    
    async def cleanup(self):
        """Cleanup database connections"""
        if self.db_manager:
            await self.db_manager.cleanup_connections()
            logger.info("HedgeFundProcessor cleanup complete")
    
    async def universal_prompt_processor(self,
                                       user_prompt: str,
                                       template_category: Optional[str] = None,
                                       currency: Optional[str] = None,
                                       entity_id: Optional[str] = None,
                                       nav_type: Optional[str] = None,
                                       amount: Optional[float] = None,
                                       use_cache: bool = True,
                                       operation_type: str = "read",
                                       write_data: Optional[Dict[str, Any]] = None,
                                       instruction_id: Optional[str] = None,
                                       execute_booking: bool = False,
                                       execute_posting: bool = False,
                                       stage_mode: str = "auto",
                                       agent_role: str = "unified",
                                       output_format: str = "json") -> Dict[str, Any]:
        """
        Universal prompt processor - core business logic shared between FastAPI and MCP
        Returns structured data instead of streaming responses for MCP compatibility
        """
        start_time = time.time()
        self.total_requests += 1
        
        try:
            # Step 0: Parse and normalize write_data (Dify often sends JSON strings or nested {data:{...}})
            if write_data and isinstance(write_data, str):
                try:
                    write_data = json.loads(write_data)
                    logger.info(f"Parsed write_data from JSON string: {type(write_data)}")
                except json.JSONDecodeError as e:
                    logger.error(f"Failed to parse write_data JSON: {e}")
                    write_data = None
            # Flatten nested payloads
            if write_data and isinstance(write_data, dict) and isinstance(write_data.get("data"), dict):
                try:
                    inner = write_data.get("data")
                    wd = inner.copy()
                    for k, v in write_data.items():
                        if k != "data":
                            wd[k] = v
                    write_data = wd
                    logger.info("Normalized nested write_data.data into flat write_data")
                except Exception as e:
                    logger.warning(f"Failed to normalize nested write_data: {e}")

            # Step 1: Analyze the user prompt
            logger.info(f"Processing prompt: {user_prompt[:100]}...")
            logger.info(f"Operation type: {operation_type}, Stage mode: {stage_mode}, Agent role: {agent_role}")

            analysis_result = analyze_prompt(user_prompt, template_category)

            # Auto-detect stage if not specified
            if stage_mode == "auto":
                if any(word in user_prompt.lower() for word in ["utilization", "capacity", "feasibility", "allocation"]):
                    detected_stage = "1A"
                elif any(word in user_prompt.lower() for word in ["booking", "deal", "murex"]):
                    detected_stage = "2"
                elif any(word in user_prompt.lower() for word in ["gl", "posting", "journal"]):
                    detected_stage = "3"
                else:
                    detected_stage = "auto"
                logger.info(f"Auto-detected stage: {detected_stage}")
            else:
                detected_stage = stage_mode
            
            # Step 2: Extract required data
            logger.info(f"Intent: {analysis_result.intent.value}, Confidence: {analysis_result.confidence}")

            extracted_data = await self.data_extractor.extract_data_for_prompt(
                analysis_result, use_cache=use_cache
            )

            # Step 2.5: Do NOT auto-promote reads to writes. Only write when explicitly requested

            # Step 2.6: AI-driven parameter decisions for write operations
            ai_decisions = {}
            if operation_type == "write":
                from .context_analyzer import context_analyzer
                from .ai_decision_engine import ai_decision_engine

                # Create context from prompt analysis
                hedge_context = await context_analyzer.create_hedge_context(
                    user_prompt, currency, amount
                )

                # Initialize AI engine with database connection
                ai_decision_engine.supabase_client = self.supabase_client

                # Get all AI decisions
                ai_decisions = await ai_decision_engine.make_all_decisions(hedge_context)
                logger.info(f"AI decisions completed with {len(ai_decisions)} intelligent parameters")

            # Step 3: Execute write operations if requested
            write_results = {}
            if operation_type != "read":
                write_results = await self.execute_write_operations(
                    operation_type, user_prompt, analysis_result, extracted_data,
                    currency=currency, entity_id=entity_id, nav_type=nav_type, amount=amount,
                    write_data=write_data, instruction_id=instruction_id,
                    execute_booking=execute_booking, execute_posting=execute_posting,
                    ai_decisions=ai_decisions  # Pass AI decisions to write operations
                )
            
            # Step 4: Build optimized context
            optimized_context = self.build_optimized_context(
                analysis_result, extracted_data, user_prompt,
                currency=currency, entity_id=entity_id, nav_type=nav_type, amount=amount,
                write_results=write_results, ai_decisions=ai_decisions
            )
            
            # Step 5: Prepare structured response
            processing_time = time.time() - start_time
            self.total_processing_time += processing_time
            
            # Prepare AI decision transparency for response
            ai_decision_summary = {}
            if ai_decisions:
                ai_decision_summary = {
                    "total_decisions": len(ai_decisions),
                    "decision_details": {
                        key: {
                            "value": decision.value,
                            "confidence": decision.confidence,
                            "reasoning": decision.reasoning[:200] + "..." if len(decision.reasoning) > 200 else decision.reasoning,
                            "category": getattr(decision, 'category', 'parameter_selection')
                        }
                        for key, decision in ai_decisions.items()
                    },
                    "context_analysis": {
                        "geographic_indicators": getattr(hedge_context, 'geographic_indicators', []) if 'hedge_context' in locals() else [],
                        "urgency_level": getattr(hedge_context, 'urgency_level', 'standard') if 'hedge_context' in locals() else 'standard',
                        "operation_type": getattr(hedge_context, 'operation_type', 'inception') if 'hedge_context' in locals() else 'inception',
                        "risk_profile": getattr(hedge_context, 'risk_profile', 'medium') if 'hedge_context' in locals() else 'medium'
                    }
                }

            # Determine top-level status (bubble write status for clarity)
            top_status = "success"
            if operation_type != "read" and isinstance(write_results, dict):
                top_status = write_results.get("status", "success")

            response = {
                "status": top_status,
                "prompt_analysis": {
                    "intent": analysis_result.intent.value,
                    "confidence": analysis_result.confidence,
                    "instruction_type": analysis_result.instruction_type,
                    "data_scope": analysis_result.data_scope,
                    "parameters": analysis_result.extracted_params
                },
                "extracted_data": extracted_data,
                "write_results": write_results if operation_type != "read" else None,
                "ai_decisions": ai_decision_summary if ai_decisions else None,  # AI Decision Transparency
                "optimized_context": optimized_context,
                "processing_metadata": {
                    "processing_time_ms": round(processing_time * 1000, 2),
                    "timestamp": datetime.now().isoformat(),
                    "cache_stats": self.data_extractor.get_cache_stats() if self.data_extractor else {},
                    "request_id": f"req_{self.total_requests}_{int(time.time())}",
                    "ai_enhanced": bool(ai_decisions),  # Flag indicating AI-driven processing
                    "stage_mode": stage_mode,
                    "detected_stage": detected_stage,
                    "agent_role": agent_role,
                    "output_format": output_format
                }
            }

            # Generate agent report if requested
            if output_format == "agent_report" and agent_role in ["allocation", "booking"]:
                try:
                    from shared.agent_report_generator import agent_report_generator
                    agent_report_generator.supabase_client = self.supabase_client

                    # Get instruction_id from write results if available
                    instruction_id = None
                    if write_results and "details" in write_results:
                        hedge_instructions = write_results["details"].get("hedge_instructions")
                        if hedge_instructions:
                            instruction_id = hedge_instructions.get("instruction_id")

                    # Generate agent-specific report
                    agent_result = await agent_report_generator.generate_report(
                        agent_type=agent_role,
                        processing_result=response,
                        instruction_id=instruction_id,
                        include_verification=True
                    )

                    # Add agent report to response
                    response["agent_report"] = agent_result.get("report", agent_result.get("formatted_report", ""))
                    response["agent_report_status"] = agent_result.get("status", "unknown")

                except Exception as e:
                    logger.error(f"Agent report generation error: {e}")
                    response["agent_report_error"] = str(e)

            # Apply intelligent response formatting
            response = self._apply_intelligent_formatting(response, user_prompt, template_category)

            logger.info(f"Processing completed in {processing_time:.2f}s")
            return response
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"Universal processor error: {e}")
            
            return {
                "status": "error",
                "error": str(e),
                "processing_metadata": {
                    "processing_time_ms": round(processing_time * 1000, 2),
                    "timestamp": datetime.now().isoformat(),
                    "request_id": f"req_{self.total_requests}_{int(time.time())}"
                }
            }
    
    def build_optimized_context(self,
                              analysis_result: PromptAnalysisResult,
                              extracted_data: Dict[str, Any],
                              user_prompt: str,
                              currency: Optional[str] = None,
                              entity_id: Optional[str] = None,
                              nav_type: Optional[str] = None,
                              amount: Optional[float] = None,
                              write_results: Optional[Dict[str, Any]] = None,
                              ai_decisions: Optional[Dict[str, Any]] = None) -> str:
        """
        Build optimized context for AI analysis - preserving exact logic from unified_smart_backend.py
        """
        try:
            # Build context sections
            context_sections = []
            
            # 1. User Intent Section
            context_sections.append(f"""
# USER REQUEST ANALYSIS
Intent: {analysis_result.intent.value}
Confidence: {analysis_result.confidence}%
Instruction Type: {analysis_result.instruction_type or 'General'}
Parameters: {json.dumps(analysis_result.extracted_params, indent=2)}
""")
            
            # 2. Data Summary Section
            total_records = sum(len(v) if isinstance(v, list) else 0 for k, v in extracted_data.items() if not k.startswith("_"))
            tables_included = [k for k in extracted_data.keys() if not k.startswith("_") and isinstance(extracted_data[k], list)]
            
            context_sections.append(f"""
# DATA SUMMARY
Tables Included: {', '.join(tables_included)}
Total Records: {total_records}
Cache Performance: {extracted_data.get('_extraction_metadata', {}).get('cache_hit_rate', 'N/A')}
""")
            
            # 3. Key Data Sections
            for table_name in tables_included[:5]:  # Limit to top 5 tables
                table_data = extracted_data.get(table_name, [])
                if table_data and len(table_data) > 0:
                    context_sections.append(f"""
# {table_name.upper().replace('_', ' ')} DATA
Records: {len(table_data)}
Sample: {json.dumps(table_data[:2], indent=2, default=str)}
""")
            
            # 4. Special Processing Results
            if "_processed_entity_groups" in extracted_data:
                entity_groups = extracted_data["_processed_entity_groups"]
                context_sections.append(f"""
# HEDGE INSTRUCTION ANALYSIS
Entity Groups Processed: {len(entity_groups)}
Processing Applied: {extracted_data.get('_processing_applied', 'N/A')}
""")
            
            # 5. AI Decision Results (if any)
            if ai_decisions:
                decision_summary = []
                for key, decision in ai_decisions.items():
                    decision_summary.append(f"- {key}: {decision.value} (confidence: {decision.confidence}%)")
                    if hasattr(decision, 'reasoning') and decision.reasoning:
                        decision_summary.append(f"  Reasoning: {decision.reasoning[:150]}...")

                context_sections.append(f"""
# AI DECISION ENGINE RESULTS
Total AI Decisions: {len(ai_decisions)}
Parameter Selections:
{chr(10).join(decision_summary)}
AI-Enhanced Processing: Enabled
""")

            # 6. Write Results (if any)
            if write_results:
                context_sections.append(f"""
# WRITE OPERATION RESULTS
Operation Type: {write_results.get('operation_type', 'N/A')}
Status: {write_results.get('status', 'N/A')}
Records Affected: {write_results.get('records_affected', 0)}
Details: {json.dumps(write_results.get('details', {}), indent=2, default=str)}
""")
            
            # 7. Context Footer
            context_sections.append(f"""
# PROCESSING CONTEXT
Original Prompt: "{user_prompt}"
Processing Timestamp: {datetime.now().isoformat()}
System: Hedge Agent v5.0.0 - MCP Compatible
""")
            
            return "\n".join(context_sections)
            
        except Exception as e:
            logger.error(f"Context building error: {e}")
            return f"""
# ERROR IN CONTEXT BUILDING
Error: {str(e)}
Original Prompt: "{user_prompt}"
Available Data Tables: {list(extracted_data.keys()) if extracted_data else 'None'}
"""
    
    async def query_supabase_direct(self,
                                  table_name: str,
                                  filters: Optional[Dict[str, Any]] = None,
                                  limit: Optional[int] = None,
                                  order_by: Optional[str] = None,
                                  use_cache: bool = True,
                                  operation: str = "select",
                                  data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Direct Supabase CRUD operations for MCP tools with full table access"""
        try:
            if not self.supabase_client:
                return {"status": "error", "error": "Supabase not connected"}

            # Validate table access for all critical hedge fund tables
            critical_tables = [
                # Stage 1A - Allocation Agent (Full CRUD)
                "hedge_instructions", "allocation_engine", "hedge_business_events",
                "entity_master", "position_nav_master", "buffer_configuration",
                "threshold_configuration", "currency_rates", "hedging_framework",

                # Stage 2 & Stage 3 - Booking Agent (Full CRUD)
                "deal_bookings", "h_stg_mrx_ext", "hedge_gl_packages",
                "hedge_gl_entries", "murex_book_config", "hedge_be_config",

                # Additional Configuration Tables (Usually Read-Only)
                "currency_configuration", "v_event_expected_deals", "v_event_acked_deals"
            ]

            logger.info(f"Executing {operation} on {table_name} with filters: {filters}")

            # Guardrails (non-blocking): produce validation warnings from table hints
            validation_warnings = []
            try:
                if self.table_hints and isinstance(self.table_hints, dict):
                    hints = self.table_hints.get(table_name)
                    if hints:
                        # Required fields for insert
                        if operation == "insert" and isinstance(data, dict):
                            req = hints.get("required_fields", [])
                            missing = [k for k in req if k not in data]
                            if missing:
                                validation_warnings.append(
                                    f"Insert on {table_name} missing required fields: {', '.join(missing)}"
                                )
                            # Status allowed check
                            status_field = hints.get("status_field")
                            allowed = hints.get("allowed_status") or []
                            if status_field and status_field in data and allowed:
                                if data[status_field] not in allowed:
                                    validation_warnings.append(
                                        f"{status_field}='{data[status_field]}' not in allowed values {allowed}"
                                    )
                        # Update should have filters and respect allowed status if provided
                        if operation == "update":
                            if not filters:
                                validation_warnings.append(
                                    f"Update on {table_name} without filters may affect multiple rows"
                                )
                            status_field = hints.get("status_field")
                            allowed = hints.get("allowed_status") or []
                            if isinstance(data, dict) and status_field and status_field in data and allowed:
                                if data[status_field] not in allowed:
                                    validation_warnings.append(
                                        f"{status_field}='{data[status_field]}' not in allowed values {allowed}"
                                    )
            except Exception as e:
                logger.debug(f"Hints validation skipped due to error: {e}")

            # Utility: normalize filters coming from Dify (may be JSON strings with PostgREST-like operators)
            def _normalize_filters(raw_filters: Any) -> Optional[Dict[str, Any]]:
                if raw_filters is None:
                    return None
                f = raw_filters
                # If filters is a JSON string, try to parse
                if isinstance(f, str):
                    try:
                        f = json.loads(f)
                    except Exception:
                        # Keep as string if not JSON; caller likely mis-specified
                        return None
                if not isinstance(f, dict):
                    return None
                return f

            def _coerce_value(v: str) -> Any:
                # Convert basic strings to bool/number where appropriate
                if isinstance(v, str):
                    low = v.lower()
                    if low == 'true':
                        return True
                    if low == 'false':
                        return False
                    # Try number
                    try:
                        if '.' in v:
                            return float(v)
                        return int(v)
                    except Exception:
                        pass
                return v

            def _apply_filters(query, flt: Dict[str, Any]):
                for key, value in flt.items():
                    # Support PostgREST style like "eq.value", "gte.value", "in.(a,b)"
                    if isinstance(value, str):
                        val = value
                        op = 'eq'
                        if val.startswith('in.(') and val.endswith(')'):
                            items = [x.strip() for x in val[4:-1].split(',') if x.strip()]
                            query = query.in_(key, [ _coerce_value(x) for x in items ])
                            continue
                        if '.' in val:
                            op, rhs = val.split('.', 1)
                        else:
                            rhs = val
                        rhsv = _coerce_value(rhs)
                        if op == 'eq':
                            query = query.eq(key, rhsv)
                        elif op in ('neq', 'ne'):
                            query = query.neq(key, rhsv)
                        elif op == 'gt':
                            query = query.gt(key, rhsv)
                        elif op == 'gte':
                            query = query.gte(key, rhsv)
                        elif op == 'lt':
                            query = query.lt(key, rhsv)
                        elif op == 'lte':
                            query = query.lte(key, rhsv)
                        elif op == 'like':
                            query = query.like(key, rhs)
                        elif op == 'ilike':
                            query = query.ilike(key, rhs)
                        else:
                            # Fallback to eq
                            query = query.eq(key, rhs)
                    elif isinstance(value, list):
                        query = query.in_(key, value)
                    else:
                        query = query.eq(key, value)
                return query

            # SELECT operation
            if operation == "select":
                query = self.supabase_client.table(table_name).select("*")

                # Apply filters
                nf = _normalize_filters(filters)
                if nf:
                    query = _apply_filters(query, nf)

                # Apply ordering
                if order_by:
                    desc = order_by.startswith('-')
                    column = order_by.lstrip('-')
                    query = query.order(column, desc=desc)

                # Apply limit
                if limit:
                    query = query.limit(limit)

                result = query.execute()
                data = result.data or []

                return {
                    "status": "success",
                    "operation": "select",
                    "table": table_name,
                    "records": len(data),
                    "data": data,
                    "hints": validation_warnings or None,
                    "timestamp": datetime.now().isoformat()
                }

            # INSERT operation
            elif operation == "insert":
                if not data:
                    return {"status": "error", "error": "Data required for insert operation"}

                # Add audit fields for critical tables
                if table_name in critical_tables:
                    data.setdefault("created_by", "HAWK_AGENT")
                    data.setdefault("created_date", datetime.now().isoformat())

                result = self.supabase_client.table(table_name).insert(data).execute()

                return {
                    "status": "success",
                    "operation": "insert",
                    "table": table_name,
                    "records": len(result.data) if result.data else 0,
                    "data": result.data,
                    "hints": validation_warnings or None,
                    "timestamp": datetime.now().isoformat()
                }

            # UPDATE operation
            elif operation == "update":
                if not data:
                    return {"status": "error", "error": "Data required for update operation"}
                if not filters:
                    return {"status": "error", "error": "Filters required for update operation"}

                # Add audit fields for critical tables
                if table_name in critical_tables:
                    data.setdefault("modified_by", "HAWK_AGENT")
                    data.setdefault("modified_date", datetime.now().isoformat())

                query = self.supabase_client.table(table_name).update(data)

                # Apply filters (with normalization)
                nf = _normalize_filters(filters)
                if nf:
                    query = _apply_filters(query, nf)

                result = query.execute()

                return {
                    "status": "success",
                    "operation": "update",
                    "table": table_name,
                    "records": len(result.data) if result.data else 0,
                    "data": result.data,
                    "hints": validation_warnings or None,
                    "timestamp": datetime.now().isoformat()
                }

            # DELETE operation
            elif operation == "delete":
                if not filters:
                    return {"status": "error", "error": "Filters required for delete operation"}

                query = self.supabase_client.table(table_name).delete()

                # Apply filters (with normalization)
                nf = _normalize_filters(filters)
                if nf:
                    query = _apply_filters(query, nf)

                result = query.execute()

                return {
                    "status": "success",
                    "operation": "delete",
                    "table": table_name,
                    "records": len(result.data) if result.data else 0,
                    "hints": validation_warnings or None,
                    "timestamp": datetime.now().isoformat()
                }

            else:
                return {"status": "error", "error": f"Unsupported operation: {operation}"}

        except Exception as e:
            logger.error(f"Database {operation} error for {table_name}: {e}")
            return {
                "status": "error",
                "operation": operation,
                "table": table_name,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    def get_system_health(self) -> Dict[str, Any]:
        """Get comprehensive system health status"""
        try:
            # Get cache stats
            cache_stats = self.data_extractor.get_cache_stats() if self.data_extractor else {}
            
            # Calculate average processing time
            avg_processing_time = (self.total_processing_time / max(self.total_requests, 1)) * 1000
            
            health_status = {
                "status": "healthy",
                "timestamp": datetime.now().isoformat(),
                "version": "5.0.0-mcp",
                "components": {
                    "supabase": "connected" if self.supabase_client else "disconnected",
                    "redis": "connected" if self.redis_client else "disconnected", 
                    "prompt_engine": "initialized" if self.prompt_engine else "not_initialized",
                    "data_extractor": "initialized" if self.data_extractor else "not_initialized"
                },
                "performance": {
                    "total_requests": self.total_requests,
                    "avg_processing_time_ms": round(avg_processing_time, 2),
                    "total_processing_time_s": round(self.total_processing_time, 2)
                },
                "cache_stats": cache_stats,
                "optimization_target": OPTIMIZATION_STATS.get("cache_hit_rate_target", "98%")
            }
            
            return health_status
            
        except Exception as e:
            logger.error(f"Health check error: {e}")
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    def manage_cache_operations(self, operation: str, **kwargs) -> Dict[str, Any]:
        """Manage cache operations"""
        try:
            if not self.data_extractor or not self.data_extractor.redis_available:
                return {
                    "status": "error",
                    "operation": operation,
                    "error": "Redis cache not available - using memory cache fallback",
                    "fallback_stats": {
                        "cache_type": "memory",
                        "redis_available": False,
                        "memory_cache_operational": True
                    },
                    "timestamp": datetime.now().isoformat()
                }
            
            if operation == "stats":
                return {
                    "status": "success",
                    "operation": "stats",
                    "cache_stats": self.data_extractor.get_cache_stats(),
                    "timestamp": datetime.now().isoformat()
                }
                
            elif operation == "clear_currency":
                currency = kwargs.get("currency")
                if not currency:
                    return {"status": "error", "error": "Currency parameter required"}
                
                cleared_keys = self.data_extractor.clear_cache_for_currency(currency)
                return {
                    "status": "success",
                    "operation": "clear_currency",
                    "currency": currency,
                    "keys_cleared": cleared_keys,
                    "timestamp": datetime.now().isoformat()
                }
                
            elif operation == "info":
                redis_info = {}
                try:
                    if self.redis_client:
                        redis_info = self.redis_client.info("memory")
                except:
                    pass
                    
                return {
                    "status": "success", 
                    "operation": "info",
                    "redis_info": redis_info,
                    "cache_stats": self.data_extractor.get_cache_stats(),
                    "timestamp": datetime.now().isoformat()
                }
            
            else:
                return {
                    "status": "error",
                    "error": f"Unknown cache operation: {operation}",
                    "available_operations": ["stats", "clear_currency", "info"],
                    "timestamp": datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"Cache management error: {e}")
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def execute_write_operations(self,
                                     operation_type: str,
                                     user_prompt: str,
                                     analysis_result: PromptAnalysisResult,
                                     extracted_data: Dict[str, Any],
                                     currency: Optional[str] = None,
                                     entity_id: Optional[str] = None,
                                     nav_type: Optional[str] = None,
                                     amount: Optional[float] = None,
                                     write_data: Optional[Dict[str, Any]] = None,
                                     instruction_id: Optional[str] = None,
                                     execute_booking: bool = False,
                                     execute_posting: bool = False,
                                     ai_decisions: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Execute database write operations based on operation type
        """
        try:
            results = {
                "operation_type": operation_type,
                "status": "success",
                "records_affected": 0,
                "details": {},
                "timestamp": datetime.now().isoformat()
            }
            
            if operation_type == "write":
                results.update(await self._execute_write(
                    user_prompt, analysis_result, write_data, currency, entity_id, nav_type, amount, ai_decisions
                ))
            
            elif operation_type == "amend":
                results.update(await self._execute_amend(
                    instruction_id, write_data, user_prompt, analysis_result
                ))
            
            elif operation_type == "mx_booking":
                results.update(await self._execute_mx_booking(
                    instruction_id, execute_booking, user_prompt
                ))
            
            elif operation_type == "gl_posting":
                results.update(await self._execute_gl_posting(
                    instruction_id, execute_posting, user_prompt
                ))
            
            # Harden status: mark as error if any table-specific error present
            details = results.get("details") or {}
            if any(k.endswith("_error") for k in details.keys()):
                results["status"] = "error"
            # If write but no rows and no tables modified, treat as error
            if operation_type == "write":
                if results.get("records_affected", 0) == 0 and not results.get("tables_modified"):
                    results["status"] = "error"
                    results["error"] = results.get("error") or "no_write_target_or_noop"

            return results
            
        except Exception as e:
            logger.error(f"Write operation error ({operation_type}): {e}")
            return {
                "operation_type": operation_type,
                "status": "error",
                "error": str(e),
                "records_affected": 0,
                "timestamp": datetime.now().isoformat()
            }
    
    async def _execute_write(self, user_prompt: str, analysis_result: PromptAnalysisResult,
                           write_data: Dict[str, Any], currency: str, entity_id: str,
                           nav_type: str, amount: float, ai_decisions: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Execute hedge instruction write operations with full table access"""
        try:
            # Validate write_data is a dictionary
            if write_data is not None and not isinstance(write_data, dict):
                raise TypeError(f"write_data must be a dictionary, got {type(write_data)}")

            results = {"records_affected": 0, "details": {}, "tables_modified": [], "verification": {}}

            # Helper function to get AI decision values
            def get_ai_value(decision_key: str, fallback_value: Any) -> Any:
                """Extract value from AI decision or use fallback"""
                try:
                    if ai_decisions and decision_key in ai_decisions:
                        decision = ai_decisions[decision_key]
                        if hasattr(decision, 'value'):
                            return decision.value
                        else:
                            logger.warning(f"AI decision {decision_key} has no 'value' attribute, using fallback")
                            return fallback_value
                    else:
                        logger.debug(f"No AI decision for {decision_key}, using fallback: {fallback_value}")
                        return fallback_value
                except Exception as e:
                    logger.error(f"Error accessing AI decision {decision_key}: {e}")
                    return fallback_value

            logger.info(f"Executing write operations with {'AI-driven' if ai_decisions else 'fallback'} parameter selection")

            # Allocation Engine Operations - AI-DRIVEN PARAMETER SELECTION
            if analysis_result.intent.value == "hedge_utilization" or (write_data and (write_data and write_data.get("target_table")) == "allocation_engine"):
                allocation_data = {
                    "allocation_id": (write_data and write_data.get("allocation_id")) or f"ALLOC_{int(time.time())}",
                    "request_id": (write_data and write_data.get("request_id")) or f"REQ_{int(time.time())}",
                    "entity_id": entity_id or (write_data and write_data.get("entity_id")) or get_ai_value("entity_id", "SYSTEM"),  # AI-driven entity selection
                    "currency_code": currency or (write_data and write_data.get("currency_code")) or (write_data and write_data.get("exposure_currency")) or "AUD",
                    "nav_type": nav_type or (write_data and write_data.get("nav_type")) or get_ai_value("nav_type", "COI"),  # AI-driven NAV type
                    "hedge_amount_allocation": amount or (write_data and write_data.get("hedge_amount_allocation")),
                    # Provide sensible defaults for common NOT NULL columns
                    "sfx_position": (write_data and write_data.get("sfx_position")) or 0,
                    "calculation_date": (write_data and write_data.get("calculation_date")) or datetime.now().date().isoformat(),
                    "executed_date": (write_data and write_data.get("executed_date")) or datetime.now().date().isoformat(),
                    "allocation_status": (write_data and write_data.get("allocation_status")) or get_ai_value("allocation_status", "Calculated"),  # AI-driven allocation status
                    "created_by": "HAWK_AGENT",
                    "created_date": datetime.now().isoformat(),
                    # Remove allocation_notes as it doesn't exist in the schema
                }

                if write_data:
                    # Only update with fields that belong to allocation_engine table
                    allowed_fields = {
                        "allocation_id", "request_id", "entity_id", "currency_code", "nav_type",
                        "sfx_position", "car_exemption_flag", "car_amount_distribution", "manual_overlay_amount",
                        "buffer_percentage", "buffer_amount", "hedged_position", "unhedged_position",
                        "hedge_amount_allocation", "available_amount_for_hedging", "waterfall_priority",
                        "allocation_sequence", "allocation_status", "execution_date", "created_by",
                        "calculation_date", "executed_date", "created_date", "event_id", "instruction_id"
                    }
                    allocation_data.update({k: v for k, v in write_data.items() if k in allowed_fields})

                try:
                    result = self.supabase_client.table("allocation_engine").insert(allocation_data).execute()
                    results["records_affected"] += len(result.data) if result.data else 0
                    results["details"]["allocation_engine"] = result.data[0] if result.data else None
                    results["tables_modified"].append("allocation_engine")
                    logger.info(f"Successfully inserted into allocation_engine: {len(result.data) if result.data else 0} records")
                except Exception as e:
                    logger.error(f"Allocation engine insert error: {e}")
                    results["details"]["allocation_engine_error"] = str(e)

            # Hedge Instructions Operations - AI-DRIVEN PARAMETER SELECTION
            if analysis_result.intent.value in ["hedge_inception", "hedge_rollover", "hedge_termination"] or (write_data and (write_data and write_data.get("target_table")) == "hedge_instructions"):
                # Derive robust currency/amount defaults if not provided
                effective_currency = currency or (write_data and write_data.get("exposure_currency"))
                effective_amount = amount or (write_data and write_data.get("hedge_amount_order"))
                try:
                    if not effective_currency:
                        effective_currency = (analysis_result.extracted_params or {}).get("currency")
                    if not effective_amount:
                        effective_amount = (analysis_result.extracted_params or {}).get("amount")
                    if ai_decisions:
                        if not effective_currency and ai_decisions.get("currency"):
                            cdec = ai_decisions.get("currency")
                            effective_currency = getattr(cdec, 'value', cdec)
                        if not effective_amount and ai_decisions.get("amount"):
                            adec = ai_decisions.get("amount")
                            effective_amount = getattr(adec, 'value', adec)
                except Exception as _e:
                    logger.debug(f"Instruction fallback param derive failed: {_e}")

                instruction_data = {
                    "instruction_id": (write_data and write_data.get("instruction_id")) or f"INST_{analysis_result.intent.value.upper()}_{int(time.time())}",
                    "instruction_type": (write_data and write_data.get("instruction_type")) or analysis_result.intent.value[6:7].upper(),  # Extract I/R/T
                    "exposure_currency": effective_currency,
                    "hedge_amount_order": effective_amount,
                    "hedge_method": (write_data and write_data.get("hedge_method")) or get_ai_value("hedge_method", "COH"),  # AI-driven hedge method
                    "instruction_status": (write_data and write_data.get("instruction_status")) or get_ai_value("instruction_status", "Received"),  # AI-driven status
                    "created_by": "HAWK_AGENT",
                    "created_date": datetime.now().isoformat(),
                    "received_timestamp": datetime.now().isoformat(),
                    "instruction_date": (write_data and write_data.get("instruction_date")) or datetime.now().date().isoformat(),
                    "trace_id": (write_data and write_data.get("trace_id")) or f"HI-INST_{int(time.time())}"
                }

                if write_data:
                    # Only update with fields that belong to hedge_instructions table
                    allowed_fields = {
                        "instruction_type", "instruction_date", "exposure_currency", "hedge_amount_order",
                        "hedge_method", "order_id", "sub_order_id", "entity_id", "nav_type", "currency_type",
                        "accounting_method", "instruction_status", "received_timestamp", "trace_id",
                        "created_by", "created_date", "msg_uid", "deal_id", "package_id"
                    }
                    instruction_data.update({k: v for k, v in write_data.items() if k in allowed_fields})

                try:
                    if not os.getenv("SUPABASE_SERVICE_ROLE_KEY"):
                        logger.warning("Attempting hedge_instructions insert without SUPABASE_SERVICE_ROLE_KEY; RLS may block writes.")
                    result = self.supabase_client.table("hedge_instructions").insert(instruction_data).execute()
                    results["records_affected"] += len(result.data) if result.data else 0
                    results["details"]["hedge_instructions"] = result.data[0] if result.data else None
                    results["tables_modified"].append("hedge_instructions")
                    logger.info(f"Successfully inserted into hedge_instructions: {len(result.data) if result.data else 0} records")
                except Exception as e:
                    # Idempotent handling for duplicate (23505): fetch by msg_uid if provided
                    err_txt = str(e)
                    logger.error(f"Hedge instructions insert error: {err_txt}")
                    results["details"]["hedge_instructions_error"] = err_txt
                    try:
                        msg_uid = (write_data and write_data.get("msg_uid")) or instruction_data.get("msg_uid")
                        if msg_uid and ("23505" in err_txt or "duplicate key value" in err_txt):
                            existing = self.supabase_client.table("hedge_instructions").select("*").eq("msg_uid", msg_uid).order("created_date", desc=True).limit(1).execute()
                            if existing.data:
                                results["details"]["hedge_instructions"] = existing.data[0]
                                results["status"] = "exists"
                                logger.info("Idempotent match found for msg_uid; returning existing instruction")
                    except Exception as _:
                        pass

            # Hedge Business Events Operations - AI-DRIVEN PARAMETER SELECTION
            if write_data and (write_data and write_data.get("target_table")) == "hedge_business_events":
                event_data = {
                    "event_id": (write_data and write_data.get("event_id")) or f"EVENT_{int(time.time())}",
                    "instruction_id": (write_data and write_data.get("instruction_id")) or (results["details"].get("hedge_instructions") or {}).get("instruction_id"),
                    "entity_id": (write_data and write_data.get("entity_id")) or get_ai_value("entity_id", "ENTITY0001"),  # AI-driven entity selection
                    "nav_type": (write_data and write_data.get("nav_type")) or get_ai_value("nav_type", "COI"),  # AI-driven NAV type
                    "currency_type": (write_data and write_data.get("currency_type")) or get_ai_value("currency_type", "Matched"),  # AI-driven currency alignment
                    "hedging_instrument": (write_data and write_data.get("hedging_instrument")) or get_ai_value("hedging_instrument", "FX_Swap"),  # AI-driven instrument selection
                    "accounting_method": (write_data and write_data.get("accounting_method")) or get_ai_value("accounting_method", "COH"),  # AI-driven accounting method
                    "notional_amount": (write_data and write_data.get("notional_amount")) or amount or 1000000,  # Use provided amount
                    "trade_date": (write_data and write_data.get("trade_date")) or datetime.now().date().isoformat(),  # Current date default
                    "business_event_type": ((write_data and write_data.get("business_event_type")) if write_data else None) or ((write_data and write_data.get("event_type")) if write_data else None) or get_ai_value("business_event_type", "Initiation"),  # AI-driven event type
                    "event_status": (write_data and write_data.get("event_status")) or get_ai_value("event_status", "Pending"),  # AI-driven event status
                    "stage_1a_status": (write_data and write_data.get("stage_1a_status")) or get_ai_value("stage_1a_status", "Pending"),  # AI-driven stage status
                    "created_by": "HAWK_AGENT",
                    "created_date": datetime.now().isoformat(),
                    "notes": (write_data and write_data.get("notes")) or f"Auto-created from: {user_prompt[:100]}"
                }

                if write_data:
                    # Only update with fields that belong to hedge_business_events table
                    allowed_fields = {
                        "event_id", "instruction_id", "entity_id", "nav_type", "currency_type",
                        "business_event_type", "event_status", "stage_1a_status", "stage_2_status", "stage_3_status",
                        "notional_amount", "allocated_amount", "booking_reference", "gl_reference",
                        "created_by", "created_date", "notes"
                    }
                    event_data.update({k: v for k, v in write_data.items() if k in allowed_fields})

                try:
                    result = self.supabase_client.table("hedge_business_events").insert(event_data).execute()
                    results["records_affected"] += len(result.data) if result.data else 0
                    results["details"]["hedge_business_events"] = result.data[0] if result.data else None
                    results["tables_modified"].append("hedge_business_events")
                    logger.info(f"Successfully inserted into hedge_business_events: {len(result.data) if result.data else 0} records")
                except Exception as e:
                    logger.error(f"Hedge business events insert error: {e}")
                    results["details"]["hedge_business_events_error"] = str(e)

            # Stage 2 Booking Operations - AI-DRIVEN PARAMETER SELECTION
            if write_data and (write_data and write_data.get("target_table")) == "deal_bookings":
                # Column alias handling (KB vs internal):
                # - deal_id ↔ booking_id (normalize to booking_id for insert)
                # - product_code ↔ portfolio_code (normalize to portfolio_code)
                # - trade_ref ↔ murex_deal_id (normalize to murex_deal_id)
                normalized_booking_id = (write_data and (write_data.get("booking_id") or write_data.get("deal_id"))) or f"BOOKING_{int(time.time())}"
                normalized_portfolio = (write_data and (write_data.get("portfolio_code") or write_data.get("product_code"))) or get_ai_value("portfolio_code", "CO_FX")
                normalized_mx_ref = (write_data and (write_data.get("murex_deal_id") or write_data.get("trade_ref")))

                booking_data = {
                    "booking_id": normalized_booking_id,
                    "instruction_id": (write_data and write_data.get("instruction_id")) or (results["details"].get("hedge_instructions") or {}).get("instruction_id"),
                    "event_id": (write_data and write_data.get("event_id")) or (results["details"].get("hedge_business_events") or {}).get("event_id"),
                    "murex_deal_id": normalized_mx_ref,
                    "portfolio_code": normalized_portfolio,
                    "notional_amount": (write_data and write_data.get("notional_amount")) or amount,
                    "currency": (write_data and write_data.get("currency")) or currency,
                    "value_date": (write_data and write_data.get("value_date")) or get_ai_value("value_date", None),
                    "maturity_date": (write_data and write_data.get("maturity_date")) or get_ai_value("maturity_date", None),
                    "booking_status": (write_data and write_data.get("booking_status")) or get_ai_value("booking_status", "Pending"),
                    "created_by": "HAWK_AGENT",
                    "created_date": datetime.now().isoformat(),
                    "notes": (write_data and write_data.get("notes")) or f"Auto-created from: {user_prompt[:100]}"
                }

                # Do not pass through KB-only fields that may not exist on this table
                # Keep payload minimal and schema-safe

                try:
                    result = self.supabase_client.table("deal_bookings").insert(booking_data).execute()
                    results["records_affected"] += len(result.data) if result.data else 0
                    results["details"]["deal_bookings"] = result.data[0] if result.data else None
                    results["tables_modified"].append("deal_bookings")
                    logger.info(f"Successfully inserted into deal_bookings: {len(result.data) if result.data else 0} records")
                except Exception as e:
                    logger.error(f"Deal bookings insert error: {e}")
                    results["details"]["deal_bookings_error"] = str(e)

            # Stage 2 External System Integration - AI-DRIVEN PARAMETER SELECTION
            if write_data and (write_data and write_data.get("target_table")) == "h_stg_mrx_ext":
                mrx_data = {
                    "message_id": (write_data and write_data.get("message_id")) or f"MRX_{int(time.time())}",
                    "instruction_id": (write_data and write_data.get("instruction_id")) or (results["details"].get("hedge_instructions") or {}).get("instruction_id"),
                    "booking_id": (write_data and write_data.get("booking_id")) or (results["details"].get("deal_bookings") or {}).get("booking_id"),
                    "message_type": (write_data and write_data.get("message_type")) or get_ai_value("message_type", "BOOKING_REQUEST"),  # AI-driven message type
                    "message_status": (write_data and write_data.get("message_status")) or get_ai_value("message_status", "QUEUED"),  # AI-driven message status
                    "message_payload": (write_data and write_data.get("message_payload")) or f"{{\"instruction_id\": \"{results['details'].get('hedge_instructions', {}).get('instruction_id')}\", \"generated_by\": \"HAWK_AGENT\"}}",
                    "processing_attempt": (write_data and write_data.get("processing_attempt")) or 1,
                    "created_by": "HAWK_AGENT",
                    "created_date": datetime.now().isoformat()
                }

                if write_data:
                    mrx_data.update({k: v for k, v in write_data.items() if k != "target_table"})

                try:
                    result = self.supabase_client.table("h_stg_mrx_ext").insert(mrx_data).execute()
                    results["records_affected"] += len(result.data) if result.data else 0
                    results["details"]["h_stg_mrx_ext"] = result.data[0] if result.data else None
                    results["tables_modified"].append("h_stg_mrx_ext")
                    logger.info(f"Successfully inserted into h_stg_mrx_ext: {len(result.data) if result.data else 0} records")
                except Exception as e:
                    logger.error(f"Murex external integration insert error: {e}")
                    results["details"]["h_stg_mrx_ext_error"] = str(e)

            # Stage 3 GL Package Header - AI-DRIVEN PARAMETER SELECTION (Hybrid with Compliance Rules)
            if write_data and (write_data and write_data.get("target_table")) == "hedge_gl_packages":
                gl_package_data = {
                    "package_id": (write_data and write_data.get("package_id")) or f"GL_PKG_{int(time.time())}",
                    "instruction_id": (write_data and write_data.get("instruction_id")) or (results["details"].get("hedge_instructions") or {}).get("instruction_id"),
                    "event_id": (write_data and write_data.get("event_id")) or (results["details"].get("hedge_business_events") or {}).get("event_id"),
                    "booking_id": (write_data and write_data.get("booking_id")) or (results["details"].get("deal_bookings") or {}).get("booking_id"),
                    "package_status": (write_data and write_data.get("package_status")) or get_ai_value("package_status", "DRAFT"),  # AI-driven package status
                    "gl_date": (write_data and write_data.get("gl_date")) or get_ai_value("gl_date", datetime.now().date().isoformat()),  # AI-driven GL date selection
                    "total_debit_amount": (write_data and write_data.get("total_debit_amount")) or 0,  # Compliance rule: Zero until calculated
                    "total_credit_amount": (write_data and write_data.get("total_credit_amount")) or 0,  # Compliance rule: Zero until calculated
                    "currency": (write_data and write_data.get("currency")) or currency,
                    "created_by": "HAWK_AGENT",
                    "created_date": datetime.now().isoformat(),
                    "notes": (write_data and write_data.get("notes")) or f"Auto-created from: {user_prompt[:100]}"
                }

                if write_data:
                    gl_package_data.update({k: v for k, v in write_data.items() if k != "target_table"})

                try:
                    result = self.supabase_client.table("hedge_gl_packages").insert(gl_package_data).execute()
                    results["records_affected"] += len(result.data) if result.data else 0
                    results["details"]["hedge_gl_packages"] = result.data[0] if result.data else None
                    results["tables_modified"].append("hedge_gl_packages")
                    logger.info(f"Successfully inserted into hedge_gl_packages: {len(result.data) if result.data else 0} records")
                except Exception as e:
                    logger.error(f"GL packages insert error: {e}")
                    results["details"]["hedge_gl_packages_error"] = str(e)

            # Stage 3 GL Entries (DR/CR Lines) - AI-DRIVEN PARAMETER SELECTION (Hybrid with Compliance Rules)
            if write_data and (write_data and write_data.get("target_table")) == "hedge_gl_entries":
                gl_entry_data = {
                    "entry_id": (write_data and write_data.get("entry_id")) or f"GL_ENTRY_{int(time.time())}",
                    "package_id": (write_data and write_data.get("package_id")) or (results["details"].get("hedge_gl_packages") or {}).get("package_id"),
                    "instruction_id": (write_data and write_data.get("instruction_id")) or (results["details"].get("hedge_instructions") or {}).get("instruction_id"),
                    "account_code": (write_data and write_data.get("account_code")) or get_ai_value("account_code", "999999"),  # AI-driven account selection
                    "debit_amount": ((write_data and write_data.get("debit_amount")) if write_data else None) or (amount if write_data and (write_data and write_data.get("entry_type")) == "DEBIT" else 0),
                    "credit_amount": ((write_data and write_data.get("credit_amount")) if write_data else None) or (amount if write_data and (write_data and write_data.get("entry_type")) == "CREDIT" else 0),
                    "entry_type": (write_data and write_data.get("entry_type")) or get_ai_value("entry_type", "DEBIT"),  # AI-driven entry type
                    "currency": (write_data and write_data.get("currency")) or currency,
                    "narrative": (write_data and write_data.get("narrative")) or get_ai_value("narrative", f"Hedge operation - {analysis_result.intent.value}"),  # AI-driven narrative
                    "business_unit": (write_data and write_data.get("business_unit")) or get_ai_value("business_unit", "TRADING"),  # AI-driven business unit
                    "profit_center": (write_data and write_data.get("profit_center")) or get_ai_value("profit_center", "FX_HEDGE"),  # AI-driven profit center
                    "created_by": "HAWK_AGENT",
                    "created_date": datetime.now().isoformat()
                }

                if write_data:
                    gl_entry_data.update({k: v for k, v in write_data.items() if k != "target_table"})

                try:
                    result = self.supabase_client.table("hedge_gl_entries").insert(gl_entry_data).execute()
                    results["records_affected"] += len(result.data) if result.data else 0
                    results["details"]["hedge_gl_entries"] = result.data[0] if result.data else None
                    results["tables_modified"].append("hedge_gl_entries")
                    logger.info(f"Successfully inserted into hedge_gl_entries: {len(result.data) if result.data else 0} records")
                except Exception as e:
                    logger.error(f"GL entries insert error: {e}")
                    results["details"]["hedge_gl_entries_error"] = str(e)

            # KB-aligned GL entries direct insert support (gl_entries)
            if write_data and (write_data and write_data.get("target_table")) == "gl_entries":
                kb_gl = {
                    "entry_id": (write_data and write_data.get("entry_id")) or f"GLENT_{int(time.time())}",
                    "package_id": (write_data and write_data.get("package_id")),
                    "be_code": (write_data and write_data.get("be_code")) or get_ai_value("be_code", "BE_FX_RE"),
                    "dr_account": (write_data and write_data.get("dr_account")),
                    "cr_account": (write_data and write_data.get("cr_account")),
                    "dr_amount_sgd": (write_data and write_data.get("dr_amount_sgd")) or 0,
                    "cr_amount_sgd": (write_data and write_data.get("cr_amount_sgd")) or 0,
                    "currency": (write_data and write_data.get("currency")) or currency,
                    "created_by": "HAWK_AGENT",
                    "created_date": datetime.now().isoformat()
                }
                try:
                    result = self.supabase_client.table("gl_entries").insert(kb_gl).execute()
                    results["records_affected"] += len(result.data) if result.data else 0
                    results["details"]["gl_entries"] = result.data[0] if result.data else None
                    results["tables_modified"].append("gl_entries")
                    logger.info(f"Successfully inserted into gl_entries: {len(result.data) if result.data else 0} records")
                except Exception as e:
                    logger.error(f"KB gl_entries insert error: {e}")
                    results["details"]["gl_entries_error"] = str(e)

            # Step: Verification via msg_uid if available
            if results["records_affected"] > 0:
                await self._verify_write_operations(results, write_data)

            return results

        except Exception as e:
            logger.error(f"Write execution error: {e}")
            raise

    async def _verify_write_operations(self, results: Dict[str, Any], write_data: Optional[Dict[str, Any]]) -> None:
        """Verify write operations via msg_uid lookup"""
        try:
            verification_results = {}

            # Check if hedge_instructions were created and verify via msg_uid
            if "hedge_instructions" in results["details"] and results["details"]["hedge_instructions"]:
                instruction_data = results["details"]["hedge_instructions"]
                instruction_id = instruction_data.get("instruction_id")

                if instruction_id:
                    # Query to verify the record exists
                    verify_query = await self.query_supabase_direct(
                        table_name="hedge_instructions",
                        filters={"instruction_id": instruction_id},
                        limit=1
                    )

                    if verify_query["status"] == "success" and verify_query["records"] > 0:
                        verification_results["hedge_instructions"] = {
                            "status": "verified",
                            "instruction_id": instruction_id,
                            "msg_uid": verify_query["data"][0].get("msg_uid"),
                            "trace_id": verify_query["data"][0].get("trace_id")
                        }
                    else:
                        verification_results["hedge_instructions"] = {
                            "status": "verification_failed",
                            "instruction_id": instruction_id,
                            "error": "Record not found after insert"
                        }

            # Check allocation_engine verification
            if "allocation_engine" in results["details"] and results["details"]["allocation_engine"]:
                allocation_data = results["details"]["allocation_engine"]
                allocation_id = allocation_data.get("allocation_id")

                if allocation_id:
                    verify_query = await self.query_supabase_direct(
                        table_name="allocation_engine",
                        filters={"allocation_id": allocation_id},
                        limit=1
                    )

                    if verify_query["status"] == "success" and verify_query["records"] > 0:
                        verification_results["allocation_engine"] = {
                            "status": "verified",
                            "allocation_id": allocation_id,
                            "allocation_run_id": verify_query["data"][0].get("allocation_run_id")
                        }
                    else:
                        verification_results["allocation_engine"] = {
                            "status": "verification_failed",
                            "allocation_id": allocation_id,
                            "error": "Record not found after insert"
                        }

            results["verification"] = verification_results
            logger.info(f"Write operation verification completed: {len(verification_results)} tables verified")

        except Exception as e:
            logger.error(f"Verification error: {e}")
            results["verification"] = {"status": "verification_error", "error": str(e)}
    
    async def _execute_amend(self, instruction_id: str, write_data: Dict[str, Any], 
                           user_prompt: str, analysis_result: PromptAnalysisResult) -> Dict[str, Any]:
        """Execute amendment operations on existing records"""
        try:
            if not write_data or not isinstance(write_data, dict):
                raise ValueError("write_data must be a dictionary for amend operations")

            target_table = write_data.get("target_table")

            # Decide whether this is an allocation_engine amend or hedge_instructions amend
            is_allocation_amend = (
                target_table == "allocation_engine"
                or ("allocation_id" in write_data)
                or ("amount" in write_data or "status" in write_data)
            )

            if is_allocation_amend:
                allocation_id = write_data.get("allocation_id")
                if not allocation_id:
                    # Fallback: if caller passed only instruction_id but intended allocation amend, require explicit allocation_id
                    raise ValueError("allocation_id required for allocation_engine amend")

                update_data = {
                    "modified_by": "HAWK_AGENT",
                    "modified_date": datetime.now().isoformat(),
                    # Do not include free-form 'notes' unless schema supports it
                }

                # Map common shorthands
                if "amount" in write_data:
                    update_data["hedge_amount_allocation"] = write_data["amount"]
                if "status" in write_data:
                    update_data["allocation_status"] = write_data["status"]

                # Merge provided fields, excluding routing and shorthand keys already mapped
                exclude_keys = {"target_table", "allocation_id", "amount", "status"}
                update_data.update({k: v for k, v in write_data.items() if k not in exclude_keys})

                # Try update with allocation_status; if column mismatch, retry with status
                try:
                    result = self.supabase_client.table("allocation_engine")\
                        .update(update_data)\
                        .eq("allocation_id", allocation_id)\
                        .execute()
                except Exception as e:
                    err = str(e).lower()
                    # Retry by swapping allocation_status <-> status if column unknown
                    try:
                        swapped = update_data.copy()
                        if "allocation_status" in swapped and "column" in err and "allocation_status" in err:
                            val = swapped.pop("allocation_status")
                            swapped["status"] = val
                        elif "status" in swapped and "column" in err and "status" in err:
                            val = swapped.pop("status")
                            swapped["allocation_status"] = val
                        result = self.supabase_client.table("allocation_engine")\
                            .update(swapped)\
                            .eq("allocation_id", allocation_id)\
                            .execute()
                    except Exception as e2:
                        logger.error(f"Allocation amend failed with both variants: {e2}")
                        raise

                return {"records_affected": len(result.data), "details": {"updated_allocation": allocation_id}}

            # Otherwise, amend hedge_instructions
            if not instruction_id:
                raise ValueError("instruction_id required for hedge_instructions amend")

            update_data = {
                "modified_by": "HAWK_AGENT",
                "modified_date": datetime.now().isoformat(),
            }
            # Exclude routing and primary-key shorthands from payload
            exclude_keys = {"target_table", "instruction_id"}
            update_data.update({k: v for k, v in write_data.items() if k not in exclude_keys})

            result = self.supabase_client.table("hedge_instructions")\
                .update(update_data)\
                .eq("instruction_id", instruction_id)\
                .execute()

            return {"records_affected": len(result.data), "details": {"updated_instruction": instruction_id}}

        except Exception as e:
            logger.error(f"Amend execution error: {e}")
            raise
    
    async def _execute_mx_booking(self, instruction_id: str, execute_booking: bool, user_prompt: str) -> Dict[str, Any]:
        """Execute Murex booking operations"""
        try:
            if not instruction_id:
                raise ValueError("instruction_id required for mx_booking operations")
            
            # Update hedge_business_events with booking status
            booking_data = {
                "stage_2_status": "In_Progress" if execute_booking else "Pending",
                "stage_2_start_time": datetime.now().isoformat() if execute_booking else None,
                "assigned_booking_model": "HAWK_AUTO_BOOKING" if execute_booking else None,
                "notes": f"Booking initiated via: {user_prompt[:100]}"
            }
            
            result = self.supabase_client.table("hedge_business_events")\
                .update(booking_data)\
                .eq("instruction_id", instruction_id)\
                .execute()
            
            # If execute_booking is True, simulate booking completion
            if execute_booking and result.data:
                completion_data = {
                    "stage_2_status": "Completed",
                    "stage_2_completion_time": datetime.now().isoformat(),
                    "murex_deal_id": f"MRX_AUTO_{int(time.time())}",
                    "booking_reference": f"BK_AUTO_{int(time.time())}",
                    "event_status": "Booked"
                }
                
                self.supabase_client.table("hedge_business_events")\
                    .update(completion_data)\
                    .eq("instruction_id", instruction_id)\
                    .execute()
            
            return {
                "records_affected": len(result.data),
                "details": {
                    "instruction_id": instruction_id,
                    "booking_status": "Executed" if execute_booking else "Queued",
                    "murex_deal_id": f"MRX_AUTO_{int(time.time())}" if execute_booking else None
                }
            }
            
        except Exception as e:
            logger.error(f"MX Booking execution error: {e}")
            raise
    
    async def _execute_gl_posting(self, instruction_id: str, execute_posting: bool, user_prompt: str) -> Dict[str, Any]:
        """Execute GL posting operations"""
        try:
            if not instruction_id:
                raise ValueError("instruction_id required for gl_posting operations")
            
            # Update hedge_business_events with GL posting status
            posting_data = {
                "stage_3_status": "In_Progress" if execute_posting else "Pending",
                "gl_posting_status": "In_Progress" if execute_posting else "Pending",
                "last_stage_3_update": datetime.now().isoformat(),
                "notes": f"GL posting initiated via: {user_prompt[:100]}"
            }
            
            result = self.supabase_client.table("hedge_business_events")\
                .update(posting_data)\
                .eq("instruction_id", instruction_id)\
                .execute()
            
            # If execute_posting is True, simulate posting completion
            if execute_posting and result.data:
                completion_data = {
                    "stage_3_status": "Completed",
                    "gl_posting_status": "Posted", 
                    "gl_package_id": f"GL_AUTO_{int(time.time())}",
                    "stage_3_attempts": 1
                }
                
                self.supabase_client.table("hedge_business_events")\
                    .update(completion_data)\
                    .eq("instruction_id", instruction_id)\
                    .execute()
            
            return {
                "records_affected": len(result.data),
                "details": {
                    "instruction_id": instruction_id,
                    "gl_status": "Posted" if execute_posting else "Queued",
                    "gl_package_id": f"GL_AUTO_{int(time.time())}" if execute_posting else None
                }
            }
            
        except Exception as e:
            logger.error(f"GL Posting execution error: {e}")
            raise

    def _apply_intelligent_formatting(self, response: Dict[str, Any], user_prompt: str, template_category: Optional[str] = None) -> Dict[str, Any]:
        """Apply intelligent response formatting based on user context and verbosity"""
        try:
            # Detect user context for formatting
            prompt_lower = user_prompt.lower()

            # Determine response style based on clues
            if any(term in prompt_lower for term in ["executive", "summary", "overview", "high-level"]):
                response_style = "executive"
            elif any(term in prompt_lower for term in ["detailed", "comprehensive", "full", "complete"]):
                response_style = "detailed"
            elif any(term in prompt_lower for term in ["technical", "debug", "trace", "error"]):
                response_style = "technical"
            elif template_category:
                response_style = template_category.lower()
            else:
                response_style = "operational"  # Default

            # Apply formatting based on style
            if response_style == "executive":
                response = self._format_executive_response(response)
            elif response_style == "detailed":
                response = self._format_detailed_response(response)
            elif response_style == "technical":
                response = self._format_technical_response(response)
            else:
                response = self._format_operational_response(response)

            # Add response metadata
            response["formatting_metadata"] = {
                "detected_style": response_style,
                "template_category": template_category,
                "verbosity_level": self._calculate_verbosity_level(response)
            }

            return response

        except Exception as e:
            logger.warning(f"Response formatting error: {e}")
            return response  # Return unmodified if formatting fails

    def _format_executive_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """Format response for executive audience - high-level summary"""
        try:
            # Create executive summary
            exec_summary = {
                "status": response.get("status"),
                "key_findings": self._extract_key_findings(response),
                "recommendations": self._extract_recommendations(response),
                "risk_highlights": self._extract_risk_highlights(response)
            }

            # Add executive summary to response
            response["executive_summary"] = exec_summary
            return response

        except Exception as e:
            logger.warning(f"Executive formatting error: {e}")
            return response

    def _format_detailed_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """Format response for detailed analysis"""
        try:
            # Add comprehensive analysis
            response["detailed_analysis"] = {
                "ai_decision_count": len(response.get("ai_decisions", {})),
                "data_sources_consulted": len(response.get("extracted_data", {})),
                "processing_efficiency": f"{response.get('processing_metadata', {}).get('processing_time_ms', 0)}ms"
            }
            return response

        except Exception as e:
            logger.warning(f"Detailed formatting error: {e}")
            return response

    def _format_technical_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """Format response for technical debugging"""
        try:
            # Add technical details
            response["technical_details"] = {
                "ai_decisions_made": len(response.get("ai_decisions", {})),
                "database_operations": response.get("write_results", {}).get("records_affected", 0),
                "processing_time": response.get("processing_metadata", {}).get("processing_time_ms", 0)
            }
            return response

        except Exception as e:
            logger.warning(f"Technical formatting error: {e}")
            return response

    def _format_operational_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """Format response for operational use - balanced detail"""
        try:
            # Add operational summary
            response["operational_summary"] = {
                "status": response.get("status"),
                "ai_enhanced": bool(response.get("ai_decisions")),
                "records_processed": response.get("write_results", {}).get("records_affected", 0)
            }
            return response

        except Exception as e:
            logger.warning(f"Operational formatting error: {e}")
            return response

    def _extract_key_findings(self, response: Dict[str, Any]) -> List[str]:
        """Extract key findings for executive summary"""
        findings = []
        if response.get("status") == "success":
            findings.append("Operation completed successfully")
        if "ai_decisions" in response:
            findings.append(f"AI-driven decisions: {len(response['ai_decisions'])}")
        return findings or ["No significant findings"]

    def _extract_recommendations(self, response: Dict[str, Any]) -> List[str]:
        """Extract recommendations for executive summary"""
        recommendations = []
        if response.get("status") == "success":
            recommendations.append("Proceed with planned operations")
        return recommendations or ["Continue with standard procedures"]

    def _extract_risk_highlights(self, response: Dict[str, Any]) -> List[str]:
        """Extract risk highlights for executive summary"""
        risks = []
        if response.get("status") == "error":
            risks.append("Operational failure detected")
        return risks or ["No significant risks identified"]

    def _calculate_verbosity_level(self, response: Dict[str, Any]) -> str:
        """Calculate response verbosity level"""
        total_keys = len(response)
        if total_keys <= 5:
            return "minimal"
        elif total_keys <= 10:
            return "standard"
        else:
            return "detailed"


# Global instance for shared use
hedge_processor = HedgeFundProcessor()
